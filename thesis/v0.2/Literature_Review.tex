%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Literature Review} 
\label{chap:Literature_Review}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\comment{
Use your literature review to help the reader to understand the value and the interest in your project.  You should look for related works already published that either support the merit of your project, or provide the background understanding/information to make your new claims.  Try to avoid writing a "catalogue" of related works (e.g this would have little of your own insight added).  Instead, describe to the reader why the related work is interesting or relevant to your own work.  What did they achieve?  What did they overlook?  It is highly recommend you finish your Literature Review with a final subsection "Summary", where you may wish to formulate highly specified research questions or hypotheses, or assert the need for your Research Methodology (next chapter).  

introduction
literature review
implementation
research methodology
results




\section{This is a section}
\subsection{This is a subsection}
\subsubsection{This is subsubsection}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure with subfigures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htb]
\centering
\begin{subfigure}[t]{.5\textwidth}
  \centering
  \includegraphics[height=4.5cm]{figures/Robot_1.jpg}
  \caption{\label{fig:left_robot} This is a robot.}
  \label{fig:theoretical}
\end{subfigure}%
\begin{subfigure}[t]{.5\textwidth}
  \centering
  \includegraphics[height=4.5cm]{figures/Robot_2.jpg}
  \caption{\label{fig:right_robot} This another robot.}
  \label{fig:practical}
\end{subfigure}
\caption{\label{fig:two_robots} These are two robots}
\label{fig:test}
\end{figure}

For example, \cite{Robots2020} discusses the two robots depicted in Figure \ref{fig:two_robots}. There is a robot in Figure \ref{fig:left_robot} and another robot in Figure \ref{fig:right_robot}.

}

This section will provide the details of the STDMA\cite{STDMA} protocol and a review of the related \textit{Distributed Model Predictive Control} (DMPC) algorithms.

\section{STDMA Explained}

\label{chap:STDMA Explained}

As previously mentioned in Section \ref{chap:Brief Introduction}, STDMA stands for \textbf{\textit{Self-organised Time-Divided Multiple Access}}, and it allows multiple agents to share the same channel for communication without centralised control.
The main assumtion of this protocol is that all agents have synchronised clocks. In practice, this is achieved through GPS\cite{STDMA_GPS}.

\textbf{The core idea of STDMA could be summarised as follows}: Represent continuous time with repeating frames that are consisted of discrete time slots.
While agents are always listening to messages from the channel, they look for free slots to occupy, and therefore use the occupied slots to broadcast their own data.

Agents using STDMA have \textbf{four phases}, which are arranged in chronological order as follows:

\begin{enumerate}
  \item \textbf{Initialisation}: Agents in this phase have not yet joined the network. The device listens to an entire frame and determines current allocation of each slot.
  \item \textbf{Network Entry}: Randomly choose an unallocated slot to broadcast their existance and reserve one slot for the next phase.  If the message sent didn't collide with others (i.e., only one agent which is myself choose to use this slot for entering), then the entering is successful. If the entering failed, reverse to the previous phase.
  \item \textbf{First Frame}: Use the slot reserved in the previous phase to reserve more slots for themselves. The number of reserved slots depends on the size of the data packet that the agent needs to send in each frame.
  \item \textbf{Continuous Operation}: Use the previously reserved slots to work normally. If some slots are released or more slots are needed, reapply for slots.
\end{enumerate}

Although the description above omitted some details (such as slot choosing strategy, calculation of required number of slots, etc.), it is clear that \textbf{the core idea of STDMA is the strategy of finding and reserving unallocated slots}.

This protocol also has several limitations, such as: \textbf{(1)} Collision in entering: In the Network Entry phase, multiple agents may accidentally choose the same unallocated slot for entering and broadcast their existance. \textbf{(2)} Capacity: When slots are not enough, conflicts would inevitably occur. 
There are some studies \cite{STDMA_improv1,STDMA_improv2} that proposed improvements for its limitations, but improving STDMA is not the focus of this project.

For detailed implementation, please see \textbf{PLACEHOLDER}.

\section{Distributed Model Predictive Control}
\comment{
This section would cover:
\begin{itemize}
  \item An brief introduction to DMPC.
  \item Why is DMPC related to this project.
  \item A summary of related latest research developments.
\end{itemize}

\subsection{Brief Introduction}
\subsubsection{What is Model Predictive Control\cite{MPC_Review1}?}

\textit{Model Predictive Control} or MPC is a form of control that based on the model and the online prediction of the controlled system.

MPC achieves its control objective by optimising the predicted outcome of the model of the controlled system\cite{MPC_Review1}.
\textbf{On every sampling instant}, a typical MPC controller would \textbf{use the current state of the plant as initial state}, and \textbf{solve a finite-horizon open-loop optimal control problem in real-time speed}, then \textbf{apply the first step of the generated control sequence} as the next control action.

The major challenge confronting the MPC algorithm is the stability issue, with several factors contributing to this problem, such as inadequate terminal conditions and limited prediction horizons.

MPC has been extensively studied and is a widely used control algorithm\cite{DMPC_Review1}. 
It's primarily employed in fields such as process industry, power electronics, building climate and energy management, and manufacturing\cite{MPC_Applications1, MPC_Applications2}.

\subsubsection{What is Distributed Model Predictive Control? \cite{DMPC_made_easy}}

% 当系统的规模达到一定大小的时候，单一的中心化MPC控制器就不能完成控制任务了(计算要求过高/只能向有限的邻居发送数据等)。所以需要DMPC：


In many scenarios, centralised controllers fail to perform control tasks due to reasons such as the computational load increasing with the expansion of system scale, 
resulting in the controller's inability to complete real-time computations, 
or the inability to dispatch control information to all components of the system within a specified time. 
In summary, \textbf{the limitations of centralised controllers made them incapable of meeting the needs of many control scenarios}, and that is why we need distributed control.

For \textit{Distributed Model Predictive Control} or DMPC, a large system is decomposed to numerous subsystems, and each of them:
\begin{itemize}
  \item Has its own dynamic.
  \item Has an MPC controller to control its actions (predict finite future states and choose the optimal current move).
  \item Could interact with and be influenced by other subsystems (usually described as \textit{variables} and \textit{constraints}).
\end{itemize}


DMPC algorithms have the following \textbf{key characteristics}, which can help readers to understand the algorithms and distinguish one algorithm from another:

\begin{itemize}
  \item \textbf{Architecture}: Different relationships between local MPC controllers. \textbf{Hierarchical}: The entire system is divided into layers, each containing several subsystems with MPC controllers, and there exists a hierarchical relationship between these layers. \textbf{Decentralised}: Each local MPC controller controls the local subsystem and don't communicate with each other. \textbf{Distributed}: Local MPC controllers cooperate to find better solution for the overall system.   
  \item \textbf{Local - Global}: How much do the local controllers know about the overall system information.
  \item \textbf{Selfish - Cooperative}: Whether the local controller only optimizes its own cost.
  \item \textbf{Iterative - Non-Iterative}: Whether the control strategy is generated through exchange and iteration among multiple local controllers.
  \item \textbf{Serial - Parallel}: Whether the local controllers transmit their messages in a specific order or multiple controllers could communicate with each other at the same time.
  \item \textbf{Synchronised - Asynchronised}: Whether the local controllers follow a shared schedule for computation and communication. 
\end{itemize}

The advantages and disadvantages of DMPC both stem from its decentralised nature. 
Like many other decentralised algorithms, \textbf{DMPC has better scalability and robustness}, can bypass communication bottlenecks, and to some extent, allows for parallel processing.
But \textbf{coordination among multiple controllers can be quite complex},
e.g., all local controllers tend to achieve Nash equilibrium for themselves, but such a tendency may not contribute to reaching Pareto optimality and might even be counterproductive for the entire system\cite{Nash_Equi}.

\subsubsection{Why is this project related to DMPC?}

The designed algorithm (detailed explanation please see Section \textbf{PLACEHOLDER}) in this project allows each agent to:
\begin{itemize}
  \item Create and join a self-organised serial communication channel.
  \item Determine the time window for planning and plan broadcasting.
  \item Use the shared channel to broadcast its own moving plan.
  \item Receive plans of other agents and use the received information to its own movement.
\end{itemize}

Which means, the presented algorithm could be classified as a DMPC algorithm, and the problem it solves could be treated as a DMPC problem.

\subsection{Latest Researches of DMPC}
}

\textit{Distributed Model Predictive Control} or DMPC is a control strategy based on constraints and optimization\cite{DMPC_Review1}.
DMPC not only inherits the capability of MPC to handle constraints but also possesses a decentralised characteristic, making it apt for addressing multi-agent swarm motion control problems, 
such as: formation control\cite{Formation_Control}, path tracking\cite{Path_Tracking}, path planning\cite{Path_Planning}, and obstacle avoidance\cite{Obstacle_Avoid}.
The commonality of such problems is that agents are subject to static constraints in a dynamic environment.



The DMPC algorithm applied in this domain can be summarised as: 
Agents communicate with other agents and solve a finite horizon optimization problem in their local MPC controller, using the solution for action control. 
The optimization criteria are typically specified based on the agent's task. 

% 罗列论文
In \cite{xin2023model}, the optimization problem is represented as 0–1 integer linear programming, and then a method based on the alternating direction method of multipliers (ADMM) is developed to coordinate AGVs and alleviate computational burden.
\cite{matouvs2022distributed} parameterises the trajectory of vehicles using polynomial splines, reducing the number of optimisation variables and achieving path tracking for vehicle platooning.
\cite{zhao2022distributed} utilises a hierarchical DMPC (HDMPC) path prediction method with a trigger mechanism to control the behaviour of drones post-obstacle avoidance, achieving a more efficient multi-agent multi-different-target search in unknown areas compared to conventional methods.
\cite{mousavi2020distributed}addresses the issues of vehicle platooning control and obstacle avoidance by modelling the motion of an agent swarm based on animal behaviours. It then transforms the local MPC problem of the agent into a closed-form convex optimization.
\cite{yang2022cooperative} utilises the grey wolf optimizer (GWO) to enhance the solution process of the DMPC controller, improving convergence speed and achieving 3D spatial path planning for multiple drones.
\cite{niu2022communication} introduces a drone path planning algorithm based on predicting the motion of surrounding drones without the need for communication. However, this comes at the cost of increased computational load.

% 可以看出，此领域的研究集中在。。。
% 但是需要注意的是，此领域的研究中通常不关注实际通信问题的解决，常常假设。。。
% 而且，其使用场景（编队控制）和本项目的目标场景有较大差别，(放解释)
From the aforementioned papers, it's evident that research in this domain predominantly focuses on the representation of the problem and the methods to solve optimisation challenges. 
The primary distinction between such algorithms and the subject of this study is:
the mode of communication between agents is often expressed as a constraint incorporated into the algorithm design\cite{novoth2020distributed}, while the actual implementation of communication is frequently omitted.
Common communication topology assumptions include: serial\cite{xin2023model}, agents communicating with others freely within a certain range\cite{mousavi2020distributed,yang2022cooperative,zheng2023distributed,zhao2022distributed}, directed graphs\cite{tang2022distributed,zang2022optimal}. On top of this, bandwidth limitations are occasionally overlooked\cite{zheng2023distributed}.
Moreover, common target scenarios (such as formation control and path tracking) differ significantly from the target scenario of this project.



Despite the aforementioned differences, the essence of the algorithm implemented in this paper aligns with that of DMPC: 
agents communicate amongst themselves and, based on the information from other agents, individually solve an optimisation problem within a finite horizon. 
A portion of the generated solution is then taken as their own control sequence.

\section{Multi Agent Path Finding}
% 本项目中的目标场景是。。。
The classical definition of the \textit{Multi Agent Path Finding} (MAPF) problem is: \textbf{Multiple agents moving step by step in an undirected graph in discrete time}\cite{MAPF_Review1}, which is precisely the target scenario of this project.

% 解决此类问题的算法可以分为几大类：
The MAPF algorithm can be divided into \textbf{several categories: classical, heuristic, rule-based, and machine learning combined}. 
Among these methods, those that are rule-based are the closest to the approach of this project.

\begin{itemize}
  \item Classical: The most popular classic MAPF algorithm is the \textit{Artificial Potential Field} (APF) algorithm, which put force fields around obstacles and goals to generate a force to guide the agents. APF methods does not guarantee collision-free and is prone to fall into local minima. There are still some research today dedicated to improving the performance of the APF algorithm, with a primary focus on optimizing the potential field function\cite{APF_improv2,APF_improv3,APF_improv4}.
  \item Heuristic algorithms mainly include A* and its variants (like  D* \cite{D*}). Heuristic algorithms mainly operate on the principle of searching for the optimal solution. Many researchers combine A* with other algorithms to achieve better performance and goes beyond the original A* \cite{A*_1,A*_2}.
  \item ML Combined: The machine learning algorithm mainly used in this field is reinforced learning, and usually combined with other non-RL techniques \cite{RL_1,RL_2,RL_3,RL_4}.
  \item Rule-based:  The rule-based methods are based on the idea that each agent follows a set of identical local decision-making principles (which means it is decentralized) and usually improve algorithm performance by modifying the rules. Most of these algorithms are bio-inspired, including \textit{Particle Swarm Optimisation} (PSO)\cite{PSO_1,PSO_2}, \textit{Genetic Algorithm} (GA)\cite{GA_1,GA_2}, \textit{Ant Colony Optimisation} (ACO)\cite{AGO_1,AGO_2}, \textit{Pegion Inspired Optimisation} (PIO)\cite{PIO_1, PIO_2} and \textit{Grey Wolf Optimisation} (GWO)\cite{GWO,GWO_1,GWO_2}. 
  Apart from these bio-inspired algorithms, there is also a category of algorithms called push-and-swap\cite{PaS_1, PaS_2}. They address problems through two kinds of actions: push (move towards the goal) and swap (exchanging positions between agents).
\end{itemize}

% 此类算法的主要权衡是
Generally speaking, researches in this area are trade-offs between completeness, computational complexity, and optimality. There is currently no perfect algorithm for this question.
Besides, direct communication between agents isn't always a key component in these methods (e.g. \cite{D*,RL_3}).
